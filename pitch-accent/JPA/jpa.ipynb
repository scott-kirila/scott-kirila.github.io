{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.4 64-bit",
   "display_name": "Python 3.8.4 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e6f90b9d7f671e9cfce5e0d2565311a3a9681f9259769709821c7c40a297c966"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Imports & Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.initializers import he_normal, he_uniform\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, GRU, LSTM, SimpleRNN, Activation, Bidirectional, TimeDistributed, LayerNormalization\n",
    "from tensorflow.keras.layers import Concatenate, Permute, Dot, Input, Multiply, RepeatVector, Lambda, Attention\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l1, l2, l1_l2\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kana_to_indices(kana_array):\n",
    "    '''\n",
    "    Converts an array of hiragana and NaN to an array of integers between 0 and 105, inclusive.\n",
    "    Only NaN and other non-hiragana are labeled as 0.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    kana_array : numpy.ndarray\n",
    "        A 2D array of syllables in hiragana, or NaN.\n",
    "        shape = (m, max_syllables)\n",
    "        m = batch size\n",
    "        max_syllables = the maximum length of a word from our batch\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        A 2D array.\n",
    "        shape = (m, max_syllables)\n",
    "    '''\n",
    "    \n",
    "    hiragana = ['<start>', '<end>', 'あ', 'い', 'う', 'え', 'お',\n",
    "                'か', 'き', 'く', 'け', 'こ',\n",
    "                'さ', 'し', 'す', 'せ', 'そ',\n",
    "                'た', 'ち', 'つ', 'て', 'と',\n",
    "                'な', 'に', 'ぬ', 'ね', 'の',\n",
    "                'は', 'ひ', 'ふ', 'へ', 'ほ',\n",
    "                'ま', 'み', 'む', 'め', 'も',\n",
    "                'や', 'ゆ', 'よ',\n",
    "                'ら', 'り', 'る', 'れ', 'ろ',\n",
    "                'わ', 'を', 'ん',\n",
    "                'が', 'ぎ', 'ぐ', 'げ', 'ご',\n",
    "                'ざ', 'じ', 'ず', 'ぜ', 'ぞ',\n",
    "                'だ', 'ぢ', 'づ', 'で', 'ど',\n",
    "                'ば', 'び', 'ぶ', 'べ', 'ぼ',\n",
    "                'ぱ', 'ぴ', 'ぷ', 'ぺ', 'ぽ',\n",
    "                'きゃ', 'きゅ', 'きょ',\n",
    "                'しゃ', 'しゅ', 'しょ',\n",
    "                'ちゃ', 'ちゅ', 'ちょ',\n",
    "                'にゃ', 'にゅ', 'にょ',\n",
    "                'ひゃ', 'ひゅ', 'ひょ',\n",
    "                'みゃ', 'みゅ', 'みょ',\n",
    "                'りゃ', 'りゅ', 'りょ',\n",
    "                'ぎゃ', 'ぎゅ', 'ぎょ',\n",
    "                'じゃ', 'じゅ', 'じょ',\n",
    "                'びゃ', 'びゅ', 'びょ',\n",
    "                'ぴゃ', 'ぴゅ', 'ぴょ',\n",
    "                'っ']\n",
    "    \n",
    "    m = kana_array.shape[0]\n",
    "    max_syllables = kana_array.shape[1]\n",
    "    \n",
    "    index_array = np.zeros(shape=(m, max_syllables), dtype='int8')\n",
    "    \n",
    "    for i in range(m):\n",
    "        \n",
    "        for j, char in enumerate(kana_array[i, :]):\n",
    "\n",
    "            if char in hiragana:\n",
    "                # add 1 to avoid setting it equal to 0\n",
    "                index_array[i, j] = hiragana.index(char) + 1\n",
    "    \n",
    "    return index_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_kana(kana_array):\n",
    "    '''\n",
    "    Tokenizes an array of hiragana.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    kana_array : numpy.ndarray\n",
    "        A 2D array of syllables in hiragana, or NaN.\n",
    "        shape = (m, max_syllables)\n",
    "        m = batch size\n",
    "        max_syllables = the maximum length of a word from our batch\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        A 2D array.\n",
    "        shape = (m, max_syllables)\n",
    "    '''\n",
    "    aa = ['あ', 'か', 'さ', 'た', 'な', 'は', 'ま', 'や', 'ら', 'わ',\n",
    "          'が', 'ざ', 'だ', 'ば', 'ぱ', 'きゃ', 'しゃ', 'ちゃ', 'にゃ',\n",
    "          'ひゃ', 'みゃ', 'りゃ', 'ぎゃ', 'じゃ', 'びゃ', 'ぴゃ', ]\n",
    "    \n",
    "    ii = ['い', 'き', 'し', 'ち', 'に', 'ひ', 'み',\n",
    "          'り', 'ぎ', 'じ', 'ぢ', 'び', 'ぴ']\n",
    "    \n",
    "    uu = ['う', 'く', 'す', 'つ', 'ぬ', 'ふ', 'む', 'ゆ', 'る', 'ぐ',\n",
    "          'ず', 'づ', 'ぶ', 'ぷ', 'きゅ', 'しゅ', 'ちゅ', 'にゅ', 'ひゅ',\n",
    "          'みゅ', 'りゅ', 'ぎゅ', 'じゅ', 'びゅ', 'ぴゅ']\n",
    "    \n",
    "    ee = ['え', 'け', 'せ', 'て', 'ね', 'へ', 'め',\n",
    "          'れ', 'げ', 'ぜ', 'で', 'べ', 'ぺ']\n",
    "    \n",
    "    oo = ['お', 'こ', 'そ', 'と', 'の', 'ほ', 'も', 'よ', 'ろ', 'ご',\n",
    "          'ぞ', 'ど', 'ぼ', 'ぽ', 'きょ', 'しょ', 'ちょ', 'にょ', 'ひょ',\n",
    "          'みょ', 'りょ', 'ぎょ', 'じょ', 'びょ', 'ぴょ']\n",
    "    \n",
    "    special = ['np.nan', '<start>', '<end>', 'っ', 'ん', 'を']\n",
    "\n",
    "    \n",
    "    m = kana_array.shape[0]\n",
    "    max_syllables = kana_array.shape[1]\n",
    "    \n",
    "    index_array = np.zeros(shape=(m, max_syllables), dtype='int8')\n",
    "    \n",
    "    for i in range(m):\n",
    "        for j, char in enumerate(kana_array[i, :]):\n",
    "\n",
    "            if char in special:\n",
    "                # add 1 to avoid setting it equal to 0\n",
    "                index_array[i, j] = special.index(char)\n",
    "            \n",
    "            elif char in aa:\n",
    "                index_array[i, j] = 6\n",
    "                \n",
    "            elif char in ii:\n",
    "                index_array[i, j] = 7\n",
    "            \n",
    "            elif char in uu:\n",
    "                index_array[i, j] = 8\n",
    "            \n",
    "            elif char in ee:\n",
    "                index_array[i, j] = 9\n",
    "                \n",
    "            elif char in oo:\n",
    "                index_array[i, j] = 10\n",
    "    \n",
    "    return index_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_mola(mola_array):\n",
    "    \n",
    "    molas = ['mola', 'accent_plain mola', 'accent_top mola']\n",
    "    \n",
    "    m = mola_array.shape[0]\n",
    "    max_syllables = mola_array.shape[1]\n",
    "    \n",
    "    cat_array = np.zeros(shape=(m, max_syllables), dtype='int8')\n",
    "    \n",
    "    for i in range(m):\n",
    "        for j, char in enumerate(mola_array[i, :]):\n",
    "            cat_value = 0\n",
    "            \n",
    "            if char == '<start>':\n",
    "                cat_value = 1\n",
    "                \n",
    "            elif char == '<end>':\n",
    "                cat_value = 2\n",
    "                \n",
    "            elif char in molas:\n",
    "                if char == 'mola':\n",
    "                    cat_value = 3\n",
    "                \n",
    "                else:\n",
    "                    cat_value = 4\n",
    "            \n",
    "            cat_array[i, j] = cat_value\n",
    "    \n",
    "    return cat_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_acc(predictions, y_test):\n",
    "\n",
    "    num_samples, word_length = predictions.shape\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    missed_zeros = 0\n",
    "    first_correct = 0\n",
    "\n",
    "    for i in range(0, num_samples):\n",
    "        \n",
    "        if predictions[i, 1] == y_test[i, 1]:\n",
    "            first_correct += 1\n",
    "\n",
    "        for j in range(1, word_length):\n",
    "\n",
    "            if y_test[i, j] != 0 and y_test[i, j] != 2:\n",
    "\n",
    "                total += 1\n",
    "\n",
    "                if predictions[i, j] == y_test[i, j]:\n",
    "\n",
    "                    correct += 1\n",
    "\n",
    "            elif y_test[i, j] == 0 and predictions[i, j] != 0:\n",
    "\n",
    "                missed_zeros += 1\n",
    "               \n",
    "    print(f\"{correct} correct out of {total} entries.\")\n",
    "    print('Accuracy:', correct / total)\n",
    "    print('First syllable correct:', first_correct / num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, model):\n",
    "\n",
    "    predictions = np.argmax(model.predict(X), axis=2)\n",
    "\n",
    "    return test_acc(predictions, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllables = pd.read_csv('syll_with_ends.csv')\n",
    "accents = pd.read_csv('acc_with_ends.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk_syll_simple = tokenize_kana(np.array(syllables.iloc[:, 1:21]))\n",
    "tk_syll_standard = kana_to_indices(np.array(syllables.iloc[:, 1:21]))\n",
    "tk_acc = tokenize_mola(np.array(accents.iloc[:, 1:21]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_simple, X_test_simple, y_train_simple, y_test_simple = train_test_split(tk_syll_simple, tk_acc, test_size=0.1, random_state=42, shuffle=True)\n",
    "X_train_standard, X_test_standard, y_train_standard, y_test_standard = train_test_split(tk_syll_standard, tk_acc, test_size=0.1, random_state=42, shuffle=True)"
   ]
  },
  {
   "source": [
    "# Sequential Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [91.49%] 10 -> 53 -> 26 -> 13 -> 5, batch_size=128\n",
    "# [91.83%] 10 -> 53 -> 53 -> 53 -> 5\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=108, output_dim=10, mask_zero=True),\n",
    "\n",
    "    Bidirectional(LSTM(units=53, return_sequences=True)),\n",
    "    LayerNormalization(),\n",
    "    Bidirectional(LSTM(units=53, return_sequences=True)),\n",
    "    LayerNormalization(),\n",
    "    Bidirectional(LSTM(units=53, return_sequences=True)),\n",
    "\n",
    "    TimeDistributed(Dense(units=5, activation='softmax'))\n",
    "])\n",
    "\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate=0.01, decay_steps=10000, decay_rate=0.98, staircase=True)\n",
    "optimizer = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "280/280 [==============================] - 46s 163ms/step - loss: 0.0989 - sparse_categorical_accuracy: 0.8757 - val_loss: 0.0739 - val_sparse_categorical_accuracy: 0.9027\n",
      "Epoch 2/10\n",
      "280/280 [==============================] - 32s 114ms/step - loss: 0.0709 - sparse_categorical_accuracy: 0.9052 - val_loss: 0.0701 - val_sparse_categorical_accuracy: 0.9040\n",
      "Epoch 3/10\n",
      "280/280 [==============================] - 35s 124ms/step - loss: 0.0645 - sparse_categorical_accuracy: 0.9123 - val_loss: 0.0652 - val_sparse_categorical_accuracy: 0.9143\n",
      "Epoch 4/10\n",
      "280/280 [==============================] - 34s 122ms/step - loss: 0.0606 - sparse_categorical_accuracy: 0.9166 - val_loss: 0.0596 - val_sparse_categorical_accuracy: 0.9187\n",
      "Epoch 5/10\n",
      "280/280 [==============================] - 34s 122ms/step - loss: 0.0566 - sparse_categorical_accuracy: 0.9221 - val_loss: 0.0574 - val_sparse_categorical_accuracy: 0.9227\n",
      "Epoch 6/10\n",
      "280/280 [==============================] - 35s 125ms/step - loss: 0.0528 - sparse_categorical_accuracy: 0.9279 - val_loss: 0.0542 - val_sparse_categorical_accuracy: 0.9264\n",
      "Epoch 7/10\n",
      "280/280 [==============================] - 35s 125ms/step - loss: 0.0489 - sparse_categorical_accuracy: 0.9334 - val_loss: 0.0509 - val_sparse_categorical_accuracy: 0.9326\n",
      "Epoch 8/10\n",
      "280/280 [==============================] - 37s 131ms/step - loss: 0.0453 - sparse_categorical_accuracy: 0.9391 - val_loss: 0.0498 - val_sparse_categorical_accuracy: 0.9353\n",
      "Epoch 9/10\n",
      "280/280 [==============================] - 36s 128ms/step - loss: 0.0421 - sparse_categorical_accuracy: 0.9435 - val_loss: 0.0471 - val_sparse_categorical_accuracy: 0.9377\n",
      "Epoch 10/10\n",
      "280/280 [==============================] - 35s 124ms/step - loss: 0.0402 - sparse_categorical_accuracy: 0.9464 - val_loss: 0.0453 - val_sparse_categorical_accuracy: 0.9409\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22d780b9d30>"
      ]
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "model.fit(x=X_train_standard,\n",
    "          y=y_train_standard,\n",
    "          epochs=10,\n",
    "          batch_size=128,\n",
    "          shuffle=True,\n",
    "          verbose=1,\n",
    "          validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20514 correct out of 22339 entries.\nAccuracy: 0.9183043108465017\nFirst syllable correct: 0.9300430156214625\n"
     ]
    }
   ],
   "source": [
    "predict(X_test_standard, y_test_standard, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "186644 correct out of 200753 entries.\nAccuracy: 0.9297196056845974\nFirst syllable correct: 0.9412622946695847\n"
     ]
    }
   ],
   "source": [
    "predict(X_train_standard, y_train_standard, model)"
   ]
  },
  {
   "source": [
    "# Attention Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class attention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, return_sequences=True):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(attention,self).__init__()\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1),\n",
    "                               initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),\n",
    "                               initializer=\"zeros\")\n",
    "        \n",
    "        super(attention,self).build(input_shape)\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        e = tf.keras.activations.tanh(tf.keras.backend.dot(x,self.W)+self.b)\n",
    "        a = tf.keras.activations.softmax(e, axis=1)\n",
    "        output = x*a\n",
    "        \n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "        \n",
    "        return tf.keras.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (90%) 108 -> 10 -> 53 -> att -> 26 -> 5\n",
    "# (88%) 108 ->  5 -> 10 -> att -> 10 -> 5\n",
    "inputs = Input(shape=(20,))\n",
    "embedding = Embedding(input_dim=108, output_dim=5, mask_zero=True)(inputs)\n",
    "pre_attention = Bidirectional(LSTM(units=10, return_sequences=True))(embedding)\n",
    "attention_layer = attention(return_sequences=True)(pre_attention)\n",
    "post_attention = LSTM(units=10, return_sequences=True)(attention_layer)\n",
    "outputs = Dense(units=5, activation='softmax')(post_attention)\n",
    "\n",
    "attention_model = Model(inputs=inputs, outputs=outputs, name='attention_model')\n",
    "\n",
    "lr_schedule = ExponentialDecay(initial_learning_rate=0.1, decay_steps=10000, decay_rate=0.98, staircase=True)\n",
    "optimizer = Adam(learning_rate=lr_schedule, beta_1=0.9, beta_2=0.999, epsilon=1e-07)\n",
    "\n",
    "attention_model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['sparse_categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "560/560 [==============================] - 16s 29ms/step - loss: 0.1207 - sparse_categorical_accuracy: 0.9484 - val_loss: 0.0996 - val_sparse_categorical_accuracy: 0.9527\n",
      "Epoch 2/10\n",
      "560/560 [==============================] - 11s 20ms/step - loss: 0.0807 - sparse_categorical_accuracy: 0.9631 - val_loss: 0.0797 - val_sparse_categorical_accuracy: 0.9630\n",
      "Epoch 3/10\n",
      "560/560 [==============================] - 12s 22ms/step - loss: 0.0801 - sparse_categorical_accuracy: 0.9628 - val_loss: 0.0854 - val_sparse_categorical_accuracy: 0.9631\n",
      "Epoch 4/10\n",
      "560/560 [==============================] - 12s 21ms/step - loss: 0.0792 - sparse_categorical_accuracy: 0.9638 - val_loss: 0.0771 - val_sparse_categorical_accuracy: 0.9641\n",
      "Epoch 5/10\n",
      "560/560 [==============================] - 12s 22ms/step - loss: 0.0771 - sparse_categorical_accuracy: 0.9646 - val_loss: 0.0750 - val_sparse_categorical_accuracy: 0.9656\n",
      "Epoch 6/10\n",
      "560/560 [==============================] - 12s 21ms/step - loss: 0.0772 - sparse_categorical_accuracy: 0.9646 - val_loss: 0.0768 - val_sparse_categorical_accuracy: 0.9650\n",
      "Epoch 7/10\n",
      "560/560 [==============================] - 12s 21ms/step - loss: 0.0771 - sparse_categorical_accuracy: 0.9649 - val_loss: 0.0779 - val_sparse_categorical_accuracy: 0.9642\n",
      "Epoch 8/10\n",
      "560/560 [==============================] - 12s 21ms/step - loss: 0.0766 - sparse_categorical_accuracy: 0.9651 - val_loss: 0.0788 - val_sparse_categorical_accuracy: 0.9655\n",
      "Epoch 9/10\n",
      "560/560 [==============================] - 12s 22ms/step - loss: 0.0789 - sparse_categorical_accuracy: 0.9641 - val_loss: 0.0801 - val_sparse_categorical_accuracy: 0.9651\n",
      "Epoch 10/10\n",
      "560/560 [==============================] - 12s 21ms/step - loss: 0.0793 - sparse_categorical_accuracy: 0.9637 - val_loss: 0.0820 - val_sparse_categorical_accuracy: 0.9630\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22c4b1954f0>"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "attention_model.fit(x=X_train_standard,\n",
    "          y=y_train_standard,\n",
    "          epochs=10,\n",
    "          batch_size=64, # 128\n",
    "          shuffle=True,\n",
    "          verbose=1,\n",
    "          validation_split=0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "19073 correct out of 22339 entries.\nAccuracy: 0.853798289986123\nFirst syllable correct: 0.8981208965361105\n"
     ]
    }
   ],
   "source": [
    "predict(X_test_standard, y_test_standard, attention_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "## Custom Attention Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train) // BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_input_size = len(input_language.word_index) + 1\n",
    "vocab_target_size = len(target_language_word.word_index) + 1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, encoder_units, batch_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.encoder_units = encoder_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = LSTM(self.encoder_units,\n",
    "                         return_sequences=True,\n",
    "                         return_state=True,\n",
    "                         recurrent_initializer='glorot_uniform')\n",
    "\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.lstm(x, initial_state=hidden)\n",
    "\n",
    "        return output, state\n",
    "\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "\n",
    "        return tf.zeros((self.batch_size, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    \n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # We get 1 at the last axis because we are applying score to self.V\n",
    "        # The shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)\n",
    "        ))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, decoder_units, batch_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.decoder_units = decoder_units\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = LSTM(self.decoder_units,\n",
    "                         return_sequences=True,\n",
    "                         return_state=True,\n",
    "                         recurrent_initializer='glorot_uniform')\n",
    "        self.fc = Dense(vocab_size)\n",
    "\n",
    "        # Used for attention\n",
    "        self.attention = BahdanauAttention(self.decoder_units)\n",
    "\n",
    "    \n",
    "    def call(self, x, hidden, encoder_output):\n",
    "        # encoder_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, encoder_output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # Passing the concatenated vector to the LSTM\n",
    "        output, state = self.lstm(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_target_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input, target, encoder_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_output, encoder_hidden = encoder(input, encoder_hidden)\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoder_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, target.shape[1]):\n",
    "            # Passing encoder_output to the decoder\n",
    "            predictions, decoder_hidden, _ = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "\n",
    "            loss += loss_function(target[:, t], predictions)\n",
    "\n",
    "            # Using teacher forcing\n",
    "            decoder_input = tf.expand_dims(target[: t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(target.shape[1]))\n",
    "        variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    encoder_hidden = encoder(initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (input, target)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(input, target, encoder_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print(f'Epoch {epoch + 1}, Batch {batch}, Loss: {batch_loss.numpy()}')\n",
    "    \n",
    "    reported_loss = total_loss / steps_per_epoch\n",
    "    print(f'Epoch {epoch + 1}, Loss: {reported_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(word):\n",
    "    \n",
    "    inputs = tf.convert_to_tensor(word)\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += targ_lang.index_word[predicted_id]\n",
    "        \n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result, word\n",
    "        \n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}
